import { Telegraf, Markup } from 'telegraf';
import { config } from 'dotenv';
import OpenAI from 'openai';
import express from 'express';
import fetch from 'node-fetch'; // pour ping Render

config();

const bot = new Telegraf(process.env.BOT_TOKEN);

const openai = new OpenAI({
  apiKey: process.env.OPENROUTER_API_KEY,
  baseURL: 'https://openrouter.ai/api/v1',
});

// âœ… Commande /start avec bouton
bot.start((ctx) => {
  ctx.reply(
    "ðŸ‘‹ Salut ! Clique sur le bouton ci-dessous pour dÃ©marrer le bot :",
    Markup.inlineKeyboard([
      Markup.button.callback("ðŸš€ DÃ©marrer UsomiBot", "wake_up_bot")
    ])
  );
});

// ðŸ“¦ GÃ¨re le bouton
bot.action("wake_up_bot", async (ctx) => {
  await ctx.answerCbQuery(); // pour retirer le "chargement" de Telegram
  await ctx.reply("â³ Activation du bot en cours...");

  // âž¤ Barre de progression simulÃ©e
  const progressBars = [
    "ðŸŸ©â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬› 10%",
    "ðŸŸ©ðŸŸ©â¬›â¬›â¬›â¬›â¬›â¬›â¬›â¬› 20%",
    "ðŸŸ©ðŸŸ©ðŸŸ©â¬›â¬›â¬›â¬›â¬›â¬›â¬› 30%",
    "ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©â¬›â¬›â¬›â¬›â¬›â¬› 40%",
    "ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©â¬›â¬›â¬›â¬›â¬› 50%",
    "ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©â¬›â¬›â¬›â¬› 60%",
    "ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©â¬›â¬›â¬› 70%",
    "ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©â¬›â¬› 80%",
    "ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©â¬› 90%",
    "ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ© 100%",
  ];

  for (let bar of progressBars) {
    await ctx.reply(bar);
    await new Promise(resolve => setTimeout(resolve, 500)); // 0.5s
  }

  // âž¤ Ping le serveur pour le rÃ©veiller
  try {
    const response = await fetch('https://usomi.onrender.com');
    if (response.ok) {
      await ctx.reply("âœ… UsomiBot est prÃªt !");
    } else {
      await ctx.reply("âš ï¸ Erreur en rÃ©veillant le bot.");
    }
  } catch (err) {
    await ctx.reply("âŒ Serveur injoignable : " + err.message);
  }
});

// ðŸ§  RÃ©pond aux messages texte
bot.on('text', async (ctx) => {
  const userMessage = ctx.message.text;

  try {
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        { role: 'system', content: "Tu es un assistant intelligent et amical." },
        { role: 'user', content: userMessage },
      ],
      max_tokens: 1000,
      temperature: 0.8,
    });

    const reply = completion.choices[0].message.content;
    ctx.reply(reply);
  } catch (error) {
    console.error("âŒ Erreur OpenRouter :", error.response?.data || error.message);
    ctx.reply("âš ï¸ Une erreur est survenue avec OpenRouter. " + (error.response?.data?.error?.message || error.message));
  }
});

// âœ… Express pour Render
const app = express();
const port = process.env.PORT || 10000;

app.get("/", (req, res) => {
  res.send("âœ… Serveur en ligne.");
});

app.listen(port, () => {
  console.log(`âœ… Serveur lancÃ© sur le port ${port}`);
});

// ðŸ”š ArrÃªt propre
process.once("SIGINT", () => bot.stop("SIGINT"));
process.once("SIGTERM", () => bot.stop("SIGTERM"));
