import { Telegraf, Markup } from 'telegraf';
import { config } from 'dotenv';
import OpenAI from 'openai';
import express from 'express';
import fetch from 'node-fetch';

config(); // Charge .env

const bot = new Telegraf(process.env.BOT_TOKEN);

const openai = new OpenAI({
  apiKey: process.env.OPENROUTER_API_KEY,
  baseURL: 'https://openrouter.ai/api/v1',
});

// === Express pour Render ===
const app = express();
const port = process.env.PORT || 10000;

// Lier le webhook Telegram Ã  Express
app.use(bot.webhookCallback('/webhook'));

// DÃ©finir le webhook Telegram
bot.telegram.setWebhook(`https://usomi.onrender.com/webhook`).then(() => {
  console.log("âœ… Webhook configurÃ© !");
});

// Route racine
app.get("/", (req, res) => {
  res.send("âœ… Serveur en ligne.");
});

// Lancer serveur Express
app.listen(port, () => {
  console.log(`âœ… Serveur Express actif sur le port ${port}`);
});

// === Commande /start avec bouton ===
bot.start((ctx) => {
  ctx.reply(
    "ğŸ‘‹ Bienvenueâ€¯! Clique sur le bouton pour dÃ©marrer le bot ğŸš€",
    Markup.inlineKeyboard([
      [Markup.button.callback("ğŸš€ DÃ©marrer le bot", "ping_backend")]
    ])
  );
});

// === Action sur le bouton ===
bot.action("ping_backend", async (ctx) => {
  await ctx.editMessageText("â³ Lancement du bot...");
  try {
    // Pinger le backend pour le rÃ©veiller
    await fetch("https://usomi.onrender.com/");

    // Simuler une barre de progression
    const steps = ["â–â–â–â–â–", "â–ƒâ–â–â–â–", "â–ƒâ–ƒâ–â–â–", "â–ƒâ–ƒâ–ƒâ–â–", "â–ƒâ–ƒâ–ƒâ–ƒâ–", "â–ƒâ–ƒâ–ƒâ–ƒâ–ƒ"];
    for (let i = 0; i < steps.length; i++) {
      await ctx.reply(`ğŸ”„ Progression : ${steps[i]}`);
      await new Promise(res => setTimeout(res, 400));
    }

    await ctx.reply("âœ… Bot prÃªt ! Pose ta question.");
  } catch (err) {
    console.error("âŒ Erreur de ping :", err.message);
    await ctx.reply("âš ï¸ Erreur lors du rÃ©veil du bot.");
  }
});

// === RÃ©ponse aux messages textes ===
bot.on('text', async (ctx) => {
  const userMessage = ctx.message.text;

  try {
    const completion = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        { role: 'system', content: "Tu es un assistant intelligent et amical." },
        { role: 'user', content: userMessage },
      ],
      max_tokens: 1000,
      temperature: 0.8,
    });

    const reply = completion.choices[0].message.content;
    ctx.reply(reply);
  } catch (error) {
    console.error("âŒ Erreur OpenRouter :", error);
    ctx.reply("âš ï¸ Erreur avec OpenRouter : " + (error.response?.data?.error?.message || error.message));
  }
});
